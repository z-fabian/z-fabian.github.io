---
layout: single
permalink: /research/
title: Research
author_profile: true
classes: wide
---

I have been working as a research assistant advised by [Mahdi Soltanolkotabi](https://viterbi-web.usc.edu/~soltanol/) in the _Foundations of Learning from Signals and Data Lab_ at USC since 2018. The lab's focus is at the intersection of theoretical foundations of learning, efficient deep learning algorithms and signal processing. We have published papers in top-tier machine learning conferences, such as [ICML](https://icml.cc/) and [NeurIPS](https://nips.cc/), and IEEE conferences such as [EUSIPCO](https://eusipco2020.org/).

My research at USC has evolved along three themes.
- My primary focus is tackling the challenges of adapting AI algorithms to basic science applications. I address issues such as training with data-scarcity (scientific data is costly!), robustness to distribution shifts, and compute efficiency of deep learning, all key to allowing a wider adoption of AI in scientific applications. We proposed techniques for [fast and accurate nano-scale imaging](/publications/2021-01-18-3d-phase-retrieval-at-nano-scale) and for [MRI reconstruction with limited training data](/publications/2021-07-01-data-augmentation-for-deep-learning).
- One of our lab's main focus is to gain a better fundamental understanding of deep learning. I had the chance to work on [transfer learning generalization bounds based on a notion of semantic distance](/publications/2020-12-12-minimax-lower-bounds-for-transfer-learning) and a [Jacobian-based theory of neural network generalization](/publications/2019-07-04-generalization-guarantees-for-neural-networks).
- I also work on [DARPA](https://www.darpa.mil/)'s [FastNICs](https://www.darpa.mil/program/fast-network-interface-cards) project under the supervision of [Salman Avestimehr](https://www.avestimehr.com/), [Mahdi Soltanolkotabi](https://viterbi-web.usc.edu/~soltanol/) and [Murali Annavaram](https://annavar.am/). The goal of this project is to rethink distributed training of machine learning models in the extreme bandwidth regime where communication cost is not a bottleneck anymore. We developed novel second-order, distributed optimization techniques that can utilize the extra bandwidth and achieve faster convergence speed than traditional first-order methods. More recently, I am exploring the potential of large bandwidth in federated learning and continual learning problems.

Prior to joining USC, I have been a research assistant under the supervision of [Se Young Yoon](https://ceps.unh.edu/person/se-young-yoon) at the [University of New Hampshire](https://www.unh.edu/) working on intelligent robotic swarm control algorithms. We published papers on [adjustable swarm autonomy](/publications/2016-12-12-coordination-of-multi-agent-leader-follower-system) and [the control of balanced leader-follower swarms](/publications/2017-12-12-coordination-of-balanced-leader-follower-swarms) at [CDC](http://cdc2016.ieeecss.org/), the top controls conference.  

## Professional service
I have served as a reviewer for the following machine learning, signal processing and controls conferences:
- Neural Information Processing Systems (NeurIPS) 2020, 2021
- International Conference on Machine Learning (ICML) 2021
- International Conference on Learning Representations (ICLR) 2020, 2021, 2022
- Sampling Theory and Applications (SampTA) 2019
- IEEE Conference on Decision and Control (CDC) 2016, 2017

# Awards and achievements
- Ming Hsieh Institute PhD Scholar 2021-2022
- Annenberg PhD Fellow, 2017-2020
- Undergraduate Academic Scholarship, 2010-2014
