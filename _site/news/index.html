<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>News | z-fabian</title>
<meta name="description" content="My personal website.">


  <meta name="author" content="Zalan Fabian">
  


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="z-fabian">
<meta property="og:title" content="News">
<meta property="og:url" content="/news/">


  <meta property="og:description" content="My personal website.">











  

  


<link rel="canonical" href="/news/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Zalan Fabian",
      "url": "/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="z-fabian Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          z-fabian
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/news/">News</a>
            </li><li class="masthead__menu-item">
              <a href="/publications/">Publications</a>
            </li><li class="masthead__menu-item">
              <a href="/assets/docs/cv_zfabian.pdf">CV</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="/">
        <img src="/assets/images/bio-photo.png" alt="Zalan Fabian" itemprop="image" class=">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="/" itemprop="url">Zalan Fabian</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Postdoc at USC Center on AI Foundations for Science.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Los Angeles, California</span>
        </li>
      

      
        
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/z-fabian/" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://linkedin.com/in/zalan-fabian" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-linkedin-in" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://scholar.google.com/citations?user=5EKjsXQAAAAJ&hl=en" rel="nofollow noopener noreferrer me"><i class="ai ai-fw ai-google-scholar-square ai-1x" aria-hidden="true"></i><span class="label">Google Scholar</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:zfabian@usc.edu" rel="me" class="u-email">
            <meta itemprop="email" content="zfabian@usc.edu" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="News">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="/news/" class="u-url" itemprop="url">News
</a>
          </h1>
          


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
        
<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/NeurIPS-logo-wide.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">10/30/2024</h2>
          

          
            <div class="archive__item-excerpt">
              <p>I’m looking forward to attending NeurIPS 2024 in Vancouver, where we will be presenting preliminary work for two of our most recent projects in workshops this year.</p>
<ul>
  <li><strong>ProPicker: Promptable Segmentation for Particle Picking in Cryogenic Electron Tomography</strong>: Cryogenic Electron Tomography (Cryo-ET) is a crucial imaging technique with the unique ability to image biological macromolecules (e.g. proteins, lipids) in their native cellular environment. We introduce a novel framework for detecting and classifying macromolecules in 3D volumes that greatly accelerates the detection pipeline, and offers rapid adaptation to new and unseen proteins. Our work will be presented as a poster at the <em>Machine Learning in Structural Biology Workshop</em>. This research is the outcome of a fantastic collaboration between our group and Simon Wiedemann and Reinhard Heckel at TUM.</li>
  <li><a href="https://arxiv.org/pdf/2409.15477"><strong>MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models</strong></a>: We present our work on stress-testing the visual reasoning capabilities of medical foundation models at two workshops: <em>AIM-FM: Advancements In Medical Foundation Models</em> and <em>Responsibly Building the Next Generation of Multimodal Foundational Models Workshop</em></li>
</ul>

<p>I’m eager to connect with colleagues and engage in discussions about AI for science, healthcare and aspects of reliability!</p>

            </div>
          

          

          

          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/mediconfusion-logo-wide.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">09/25/2024</h2>
          

          
            <div class="archive__item-excerpt">
              <p>Excited to announce <strong>MediConfusion</strong>, our new medical evaluation benchmark for Multimodal Large Language Models (MLLMs).
MediConfusion is a challenging medical Visual Question Answering (VQA) benchmark designed to test the vision capabilities of medical MLLMs. Our findings reveal several critical failures:</p>
<ul>
  <li>All available models — both open-source and proprietary, even those specifically designed for medical applications — performed below random guessing on MediConfusion.</li>
  <li>State-of-the-art models are easily confused by image pairs that are otherwise visually dissimilar and clearly distinct for medical experts.</li>
  <li>These results raise serious questions about the reliability of current medical MLLMs for healthcare deployment.</li>
</ul>

<p>We also identified common patterns of model failure, which we hope will guide the development of a new generation of more trustworthy and reliable MLLMs in healthcare. This is just the first step towards a more comprehensive benchmark with more medical subspecialties, categories, and samples yet to come, so stay tuned!</p>

            </div>
          

          
            <a href="https://arxiv.org/pdf/2409.15477" class="btn btn--primary">ArXiv</a>
          

          
            <a href="https://sites.usc.edu/aif4s/2024/09/25/mediconfusion/" class="btn btn--primary">Blog</a>
          

          
            <a href="https://github.com/MShahabSepehri/MediConfusion" class="btn btn--primary">Code</a>
          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/ICML-logo-wide.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">07/15/2024</h2>
          

          
            <div class="archive__item-excerpt">
              <p>I’m pleased to share that I’ll be attending ICML 2024 in Vienna, with two main track papers accepted this year, including a Spotlight, as well as some exciting preliminary work.</p>
<ul>
  <li><a href="https://openreview.net/pdf/c0010f659cac8e5485a5020c696bf0520bbb0ead.pdf"><strong>[Spotlight] Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models</strong></a>: In this work, we improve the efficiency of diffusion-based inverse problem solvers by adapting the sampling trajectory to the degraded input image. We do this by first estimating how severly degraded the corrupted input is, then we use this degradation severity to find an optimal starting time in the diffusion process. We achieve up to 10x speedup in sampling speed as well as improved reconstruction quality.</li>
  <li><a href="https://openreview.net/pdf/2e0d4e76462e0bf7282e6918745a486eb43eae21.pdf"><strong>DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency</strong></a>: We propose a novel diffusion-based framework for inverse problem solving: we model image corruption as a stochastic degradation process that gradually degrades and noises the clean image. We learn to reverse this corruption process and incrementally add more and more detail back to images. By leveraging early-stopping, we can flexibly trade off perceptual quality (how ‘nice’ the image looks) for better distortion metrics (faithfulness to the observation) or vice versa.</li>
  <li><a href="https://arxiv.org/pdf/2403.17902"><strong>[Workshop] Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models</strong></a>: We explore the potential of Structured State Space Models for image restoration. We introduce a novel architecture that converts the input image into a collection of sequences and leverages state space models as the fundamental computation block. We observe great improvements in model eficiency: reduced compute cost, lower GPU memory requirements and faster inference, and performance on par with state-of-the-art architectures! Our work will be presented at the Next Generation of Sequence Modeling Architectures Workshop.</li>
</ul>

<p>Looking forward to productive discussions and new connections!</p>

            </div>
          

          

          

          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/NeurIPS-logo-wide.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">11/20/2023</h2>
          

          
            <div class="archive__item-excerpt">
              <p>As NeurIPS 2023 is coming up, I am happy to share that we are going to present some interesting results both in the main track and at various workshops.</p>
<ul>
  <li><a href="https://arxiv.org/abs/2311.07784"><strong>A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks</strong></a>: Our work on mitigating catastrophic forgetting in continual learning in a federated setting using a data-free generative approach will be presented as a poster in the main track.</li>
  <li><a href="https://arxiv.org/abs/2309.06642"><strong>Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models</strong></a>: Our lab is going to participate in the Deep Learning and Inverse Problems Workshop presenting our recent work on sample-adaptive image reconstruction leveraging latent diffusion models.</li>
  <li><a href="https://arxiv.org/abs/2311.01064"><strong>Multimodal Foundation Models for Zero-shot Animal Species Recognition in Camera Trap Images</strong></a>: My collaboration with Microsoft’s AI for Good lab on zero-shot wildlife recognition using multimodal foundation models will be presented as a poster at the Instruction Workshop.</li>
</ul>

            </div>
          

          

          

          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/wildmatch-overview.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">11/02/2023</h2>
          

          
            <div class="archive__item-excerpt">
              <p>After a summer of amazing research collaboration with Microsoft’s AI for Good lab on advancing wildlife conservation efforts, we are eager to announce that a preliminary version of our work has been published on arXiv. We investigate how multimodal foundation models can aid biologists in automatically identifying animal species in camera trap imagery. Our proposed technique, WildMatch, can greatly reduce the cost of image analysis, as it requires no expert-labelled training data. We leverage the rich visual understanding capabilities of pre-trained vision-language foundation models, and adapt them for detailed visual description generation of animals. Then, we find the closest match in an external knowledge base of animal descriptions built from Wikipedia and other publicly available sources. This is still a work in progress with additional results coming soon.</p>

            </div>
          

          
            <a href="https://arxiv.org/abs/2311.01064" class="btn btn--primary">arXiv</a>
          

          

          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/ms-logo.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">01/15/2023</h2>
          

          
            <div class="archive__item-excerpt">
              <p>I am thrilled to share that I’m joining Microsoft’s <strong>AI for Good Lab</strong> as a research intern for the summer of 2023. Microsoft’s AI for Good initiative is at the forefront of using artificial intelligence to address some of the world’s most pressing global challenges to the environment, humanitarian issues and healthcare. In collaboration with Zhongqi Miao, I will be working on leveraging multimodal foundation models to advance wildlife conservation efforts. I’m excited to learn and make a positive impact with AI for Good researchers!</p>

            </div>
          

          

          

          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/NeurIPS-logo.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">09/14/2022</h2>
          

          
            <div class="archive__item-excerpt">
              <p>Our work <em>HUMUS-Net: a Transformer-convolutional hybrid model for accelerated MRI reconstruction</em> has been accepted for NeurIPS 2022. I am looking forward to sharing our work and interacting with other researchers in the field in person for the first time in a while at NeurIPS.</p>

            </div>
          

          
            <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/a1bb3f96e255ae1e04325ae166bcef0f-Paper-Conference.pdf" class="btn btn--primary">Paper</a>
          

          
            <a href="/assets/slides/HUMUS_Net_slides.pdf" class="btn btn--primary">Slides</a>
          

          
            <a href="https://github.com/z-fabian/HUMUS-Net" class="btn btn--primary">Code</a>
          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/CM2022-microscopy.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">09/12/2022</h2>
          

          
            <div class="archive__item-excerpt">
              <p>I am participating in <a href="http://www.ipam.ucla.edu/programs/long-programs/computational-microscopy/">IPAM’s Computational Microscopy</a> long program at UCLA as a Graduate Visiting Researcher. This program brings together leading experts in the fields of applied mathematics, physics, biology, materials science and engineering in order to encourage debate and collaboration on modern microscopy techniques, such as coherent diffraction imaging, super-resolved fluorescence microscopy (2014 Nobel prize) and a special focus on cryo-electron microscopy (cryo-EM, 2017 Nobel prize). These methods result in high-dimensional, multimodal and extremely noisy data, where extracting useful information, and eventually scientific knowledge is challenging. Deep learning has enormous potential in tackling these challenges that may lead to breakthroughs in materials science, quantum devices and drug discovery.</p>

            </div>
          

          

          

          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/amazon-logo.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">05/19/2022</h2>
          

          
            <div class="archive__item-excerpt">
              <p>I am very excited to share that I will be joining Amazon’s Alexa Perceptual Technologies as an Applied Scientist Intern for the summer under the mentorship of Rajath Kumar. We are going to work on improving wake word verification models through semi-supervised learning techniques and data augmentation. I am looking forward to collaborating with Amazon researchers and learning more about their work.</p>

            </div>
          

          

          

          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/research-festival2021-poster-small.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">10/29/2021</h2>
          

          
            <div class="archive__item-excerpt">
              <p>Today we hosted the <a href="https://minghsiehece.usc.edu/11th-annual-mhi-research-festival/">11th Annual Research Festival</a> at the Ming Hsieh ECE department with close to 100 posters and guided lab tours organized by the <a href="https://sites.usc.edu/disc/">Dynamic Imaging Science Center (DISC)</a> showcasing their new, unique high-performance low-field MRI scanner. My poster on <a href="/publications/2021-07-01-data-augmentation-for-deep-learning">MRAugment</a> has won the <em>Best Poster - Honorable Mention</em> award.</p>

            </div>
          

          
            <a href="/assets/posters/research_festival2021_poster.pdf" class="btn btn--primary">Poster</a>
          

          

          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/mhi-scholars.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">09/28/2021</h2>
          

          
            <div class="archive__item-excerpt">
              <p>I have been selected as a <a href="https://minghsiehece.usc.edu/mhi-home/mhi-mhi-scholars/">Ming Hsieh PhD Scholar</a> for 2021-2022 along with Haleh Akrami, Hefei Liu, Rodrigo Lobos, Qinyi Luo and Qiaochu Zhang. We are going to work together to organize professional events and support our PhD community. You can find the slides for my presentation <em>Overcoming the data bottleneck in AI for the sciences</em> presented at the MHI Scholar Finalist Talk Competition below.</p>

            </div>
          

          
            <a href="/assets/slides/mhi2021_slides.pdf" class="btn btn--primary">Finalist Talk Slides</a>
          

          

          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/mraugment-flowchart.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">07/01/2021</h2>
          

          
            <div class="archive__item-excerpt">
              <p>Our paper <a href="/publications/2021-07-01-data-augmentation-for-deep-learning">Data augmentation for deep learning based accelerated MRI reconstruction with limited data</a> has been accepted for short talk at ICML 2021. In this work we propose <strong>MRAugment</strong>, a data augmentation pipeline for MRI that improves MRI image reconstruction quality when training data is scarce and helps training more robust deep learning models against various forms of distributions shift (different scanner models, anatomies) and hallucinations.</p>

            </div>
          

          
            <a href="http://proceedings.mlr.press/v139/fabian21a/fabian21a.pdf" class="btn btn--primary">Paper</a>
          

          
            <a href="https://github.com/z-fabian/MRAugment" class="btn btn--primary">Code</a>
          

          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/3d-recon.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">01/18/2021</h2>
          

          
            <div class="archive__item-excerpt">
              <p>Our paper <a href="/publications/2021-01-18-3d-phase-retrieval-at-nano-scale">3D phase retrieval at nano-scale via accelerated Wirtinger flow</a> has been accepted at EUSIPCO 2020. High-resolution imaging of small 3D structures is an important problem in biology (protein complexes) and microelectronics (chip manufacturing). Our work introduces a fast and accurate algorithm that can recover the 3D structure of such objects more accurately and from fewer projections than previous techniques.</p>

            </div>
          

          
            <a href="https://ieeexplore.ieee.org/abstract/document/9287703" class="btn btn--primary">Conference Paper</a>
          

          
            <a href="https://arxiv.org/pdf/2002.11785.pdf" class="btn btn--primary">Extended arXiv</a>
          

          

        </div>
      </div>
    </div>
  

</div>

<div class="feature__wrapper">

  
    <div class="feature__item--left">
      <div class="archive__item">
        
          <div class="archive__item-teaser">
            <img src="/assets/images/transfer-lowerbounds.png" alt="" />
            
          </div>
        

        <div class="archive__item-body">
          
            <h2 class="archive__item-title">12/12/2020</h2>
          

          
            <div class="archive__item-excerpt">
              <p>Our work <a href="/publications/2020-12-12-minimax-lower-bounds-for-transfer-learning">Minimax lower bounds for transfer learning with linear and one-hidden layer neural networks</a> has been accepted at NeurIPS 2020 for poster presentation. We investigate how the number of source samples and the distance between datasets impact model generalization on a target dataset. We introduce a novel metric, the so called <em>transfer distance</em>, that quantifies how challenging it is to transfer knowledge from one dataset to another.</p>

            </div>
          

          
            <a href="https://proceedings.neurips.cc/paper/2020/file/151d21647527d1079781ba6ae6571ffd-Paper.pdf" class="btn btn--primary">Paper</a>
          

          

          

        </div>
      </div>
    </div>
  

</div>

        
      </section>

      <footer class="page__meta">
        
        


        

      </footer>

      

      
    </div>

    
  </article>

  
  
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://github.com/z-fabian/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://linkedin.com/in/zalan-fabian" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin-in" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.com/citations?user=5EKjsXQAAAAJ&hl=en" rel="nofollow noopener noreferrer"><i class="ai ai-fw ai-google-scholar-square ai-1x" aria-hidden="true"></i> Google Scholar</a></li>
        
      
        
          <li><a href="mailto:zfabian@usc.edu" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Zalan Fabian. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
